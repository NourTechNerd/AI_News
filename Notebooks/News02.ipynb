{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.Surya OCR :\n",
    "\n",
    "- This OCR engine perfome very well\n",
    "- <a href=\"https://github.com/VikParuchuri/surya?tab=readme-ov-file\">SuryaOCR Github</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Speech-Speech models :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Quetion is  : Why not using a pipeline like : **Speech-Text-->LLM-->Text-Speech** instead of : **End-To-End Speech model**\n",
    "\n",
    "|End-To-End Speech model |Speech-Text-->LLM-->Text-Speech|\n",
    "|-|-|\n",
    "|More Fast while the inferencing phase|Slower because we have 3 Components in the pipeline|\n",
    "|Better Response, bacause the model trained to undestand `Tons`,`Emotions` in speech|Less Accurate due to the convesion Speech-Text|\n",
    "|Hard to Train/fine tune it or to prepare data for it|More Easy to implement|\n",
    "|Not Flexible|Modular and Flexible (I can Swith between LLMs or the tow outher components easly)|\n",
    "\n",
    "### Phonetic tokens :\n",
    "\n",
    "- A standard way to represent **Sounds** in Speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta News :\n",
    "\n",
    "<a href=\"https://ai.meta.com/blog/fair-news-segment-anything-2-1-meta-spirit-lm-layer-skip-salsa-lingua/\">Details</a>\n",
    "\n",
    "### SAM 2.1\n",
    "\n",
    "- Improved version of the SAM 2 (Segment Anything Model) regards to performace metrics.\n",
    "\n",
    "### Spirit LM (7b):\n",
    "\n",
    "- A state of the art Mutlitmodal LLM (Input can be either Text/Speech ).\n",
    "- Use an Llama2 Architecture Combined with Speech encoder and Decoder.\n",
    "- Use the Phonetic tokens.\n",
    "\n",
    "### Layer Skip:\n",
    "\n",
    "- A Technique to reduce inference time of LLMs, by Using a subset of layers for generation and the subsequent for verefication and correction.\n",
    "- Accelerates inference up to 1.7x (That means if your model inference time is 10s, it will reduce it to 5.88 s).\n",
    "- <a href=\"https://github.com/facebookresearch/LayerSkip\">Github</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.HuggingChat :\n",
    "\n",
    "<a href=\"https://huggingface.co/chat/\">HuggingChat</a>\n",
    "\n",
    "- A HunggingFace Chat interface build on top of an Agent.\n",
    "- We can Tools as we can.\n",
    "- Free.\n",
    "- Hunggingface Provides free API to run Models. <a href=\"https://huggingface.co/docs/api-inference/index\">Details here</a>\n",
    "- There are some restrictions on the API for Large Models (8b+)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.Upstage Document Parser :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert complex documents to LLM-readable formats in Human-reading ordre.\n",
    "- Works on different formats (Image,Pdf,docx,pptx..) and convert them into HTML/Markdown.\n",
    "- Better than using an regular OCR. Why ? because as of my experience,an OCR woll return a normal text dosn't reflects the **Layout** of the document.\n",
    "- Free 10$ to test it, every API request costs you 0.01$.\n",
    "\n",
    "- <a href=\"https://console.upstage.ai/docs/getting-started/overview\">Mother company</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.EMBED 3:\n",
    "\n",
    "- A New `Mutlimodal-Embedding` model form Cohere can generate enbeddings for both imges and Text different from the single-embedding model.\n",
    "- One single Vector Store for both image and Text embeddings.\n",
    "<img style=\"width: 600px; margin: 10px; align: center\" src=\"Images/16_Image1.png\" alt=\"EMBED_3.png\">\n",
    "\n",
    "<a href=\"https://cohere.com/blog/multimodal-embed-3\">Details</a>\n",
    "\n",
    "- The text and image encoders within Embed 3 share a unified latent space so we can store them at the same Vector Store database.\n",
    "\n",
    "- Use cases include : \n",
    "   - A part of E-commerce Search Engines, where the user describe the product features then the retriver return relevent Products from the Store !!\n",
    "   - In RAG pipelines to represent Textuel and visual Content from the Docs.\n",
    "\n",
    "<a href=\"https://docs.cohere.com/\">Docs</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.OmniParser:\n",
    "\n",
    "- A Screenshot Parsing Model built on Yolov8 and BLIP-2 models.\n",
    "- Converts an image to a Structured description with Clickable items,Text Areas...\n",
    "- Plugin-ready for Other Vision Language Models\n",
    "<img style=\"width: 600px; margin: 10px; align: center\" src=\"Images/17_Image1.png\" alt=\"OmniParser.png\">\n",
    "\n",
    "<a href=\"https://huggingface.co/microsoft/OmniParser\">HuggingFace</a>\n",
    "<a href=\"https://microsoft.github.io/OmniParser/\">Details</a>\n",
    "<a href=\"https://github.com/microsoft/OmniParser/tree/master\">Github</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.Synch and Asynch :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1 Synchrone functions :\n",
    "\n",
    "- if we have two independent functions Task1(), Task2() in the main we call Task1() first so Task2() should wait until Task1() completed !! ----> Wasting Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Coffee Start\n",
      "Preparing Coffee Completed\n",
      "Eating Donut Start\n",
      "Eating Donut Completed\n",
      "Time to complete : 5.01620626449585\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "def Prepare_Coffee():    \n",
    "    print(\"Preparing Coffee Start\")\n",
    "    time.sleep(3)\n",
    "    print(\"Preparing Coffee Completed\")\n",
    "\n",
    "def Eating_Donut():    \n",
    "    print(\"Eating Donut Start\")\n",
    "    time.sleep(2)\n",
    "    print(\"Eating Donut Completed\")\n",
    " \n",
    "def Synchrone():  \n",
    "    start = time.time()\n",
    "    Prepare_Coffee()\n",
    "    Eating_Donut()\n",
    "    end = time.time()\n",
    "    print(\"Time to complete :\" , end-start)\n",
    "\n",
    "Synchrone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2 Asynchrone functions :\n",
    "\n",
    "- if we have two independent functions Task1(), Task2() when we run the both start at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coroutine function\n",
    "\n",
    "- Coroutine function returs a coroutine object\n",
    "- Couroutine function dosn't start executing until we await it.\n",
    "- We use `await` keyword to wait for the coroutine to complete its execution. We use it only inside the `async` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def fetch_data1(id ,delay):\n",
    "    print(f\"Fetching by Couroutine {id}...\")\n",
    "    await asyncio.sleep(delay)\n",
    "    print(f\"Data Fetched by Couroutine {id}\")\n",
    "    return f\" aabbabbaa{id}\"\n",
    "\n",
    "async def main2():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run tasks concurrently and wait for all to complete\n",
    "    result1, result2 = await asyncio.gather(\n",
    "        fetch_data1(1, 2),\n",
    "        fetch_data1(2, 2),\n",
    "    )\n",
    "    \n",
    "    result3 = await asyncio.gather( fetch_data1(3, 2))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"Received data:\", {result1}, {result2}, {result3})\n",
    "    print(f\"Execution time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "asyncio.run(main2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can imagine `asyncio.gather` like a function that create a queue of coroutines objects , then lunch them concurrently and wait for all to complete.\n",
    "\n",
    "- How does it work ? Asyncio use a concept called `Event Loop`, wich manages the execution of the coroutines.The Task 1 take place in the event loop to be executed , because it contains a waiting step event loop will automatically switch to the next task in the queue.\n",
    "\n",
    "- Here we start concurrently two coroutines , then we wait for both to complete.After that we start the 3rd coroutine and wait for it to complete.\n",
    "\n",
    "<img src=\"Images/18_Image1.png\" width=\"500\" title=\"hover text\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19.Docling from IBM :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/19_Image1.png\"  height=\"200\">\n",
    "\n",
    "- It's a useful tool for parsing various types of docs(pdf, docx, txt,ppt, etc.) to Markdown or Json format.\n",
    "- Run Locally.\n",
    "- <a href=\"https://github.com/DS4SD/docling\">Docling Github</a>\n",
    "- <a href=\"https://ds4sd.github.io/docling/\">Docling Documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"https://arxiv.org/pdf/2408.09869\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
